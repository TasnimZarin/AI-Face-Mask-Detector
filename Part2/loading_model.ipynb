{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"HCY2b6i6eJvO"},"outputs":[],"source":["import os\n","from os.path import exists\n","from pathlib import Path\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","import torchvision.transforms as transforms\n","import torchvision.datasets as datasets\n","from shutil import copyfile\n","import torchvision\n","from PIL import Image\n","import io\n","import random\n","import matplotlib.pyplot as plt\n","import numpy as np\n","from torch.autograd import Variable\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.utils.data import random_split\n","from torch.utils.data import TensorDataset, ConcatDataset, DataLoader\n","import pandas as pd\n","from torchvision.transforms import ToTensor, Lambda\n","from torchvision.io import read_image\n","import matplotlib.pyplot as plt\n","import torch.optim as optim\n","from sklearn.metrics import accuracy_score\n","from sklearn.metrics import plot_confusion_matrix\n","from skorch import NeuralNetClassifier\n","import sklearn"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qIDDhthAeJvW"},"outputs":[],"source":["class CustomImageDataset():\n","    def __init__(self, annotations_file, img_dir,\n","            transform=torchvision.transforms.Compose([\n","                transforms.ToPILImage(),\n","                transforms.Grayscale(num_output_channels=3),\n","                transforms.ToTensor(),\n","                #Resizing all of the imagesin the dataset to (240,240)\n","                transforms.Resize((240,240)),\n","                #Normalization using the mean and variance over the whole dataset\n","                transforms.Normalize((0.5352, 0.5352, 0.5352),(0.3009, 0.3009, 0.3009))]),\n","                 #Lambda function is used for turning the integer into a tensor\n","            target_transform=Lambda(lambda y: torch.zeros(5, dtype=torch.float).scatter_(0, torch.tensor(y), value=1))):\n","        self.img_labels = pd.read_csv(annotations_file)   #Dataaset labels\n","        self.img_dir = img_dir      #Dataset image path     \n","        self.transform = transform   \n","        self.target_transform = target_transform\n","        \n","\n","    def __len__(self):\n","        return len(self.img_labels)\n","\n","    def __getitem__(self, idx):\n","        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n","        if(os.path.exists(img_path)):\n","            image = read_image(img_path)\n","            label = torch.tensor(int(self.img_labels.iloc[idx, 1]))\n","            if self.transform:\n","                image = self.transform(image)\n","            image.shape\n","            return image, label"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8pvv3oKkeJvZ"},"outputs":[],"source":["# os.chdir(\"/content/drive/MyDrive/COMP-6721(AI_project)/Submission\");\n","# path = \"C:\\Concordia_Program\\AI\\project_part1\\Sample images\"\n","# model = torch.load('C:\\Concordia_Program\\AI\\project_phase1\\ourmodel.pth')\n","# model.load_state_dict(torch.load(\"/content/drive/MyDrive/COMP-6721(AI_project)/Submission/ourmodel.pth\"))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"07VCslpeeJvY"},"outputs":[],"source":["batch_size = 32\n","validation_split = .2\n","shuffle_dataset = True\n","random_seed= 42   #Preventing from random events in each run\n","sample_dataset1=CustomImageDataset(annotations_file=r'C:\\Concordia_Program\\AI\\project_part2\\female_test.csv', img_dir=r'C:\\Concordia_Program\\AI\\project_part2\\female_test')\n","sample_dataset2=CustomImageDataset(annotations_file=r'C:\\Concordia_Program\\AI\\project_part2\\male_test.csv', img_dir=r'C:\\Concordia_Program\\AI\\project_part2\\male_test')\n","sample_dataset3 = CustomImageDataset(annotations_file=r'C:\\Concordia_Program\\AI\\project_part1\\sample-label.csv', img_dir=r'C:\\Concordia_Program\\AI\\project_part1\\Sample images')\n","\n","test_loader1 = torch.utils.data.DataLoader(sample_dataset1, batch_size,shuffle=False)\n","test_loader2 = torch.utils.data.DataLoader(sample_dataset2, batch_size,shuffle=False)\n","test_loader3 = torch.utils.data.DataLoader(sample_dataset3, batch_size,shuffle=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"L1ishSnRiNR4"},"outputs":[],"source":["class CNN(nn.Module):\n","    def __init__(self):\n","        super(CNN, self).__init__()\n","        self.conv_layer = nn.Sequential(\n","            nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1), #First layer\n","            nn.BatchNorm2d(32),                 #Batch normalization\n","            nn.LeakyReLU(inplace=True),         #Leaky ReLU activation function\n","            nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, padding=1), #Second layer\n","            nn.BatchNorm2d(32),                #Batch normalization\n","            nn.LeakyReLU(inplace=True),        #Leaky ReLU activation function\n","            nn.MaxPool2d(kernel_size=2, stride=2),     #Max pooling\n","            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1), #Third layer\n","            nn.BatchNorm2d(64),              #Batch normalization\n","            nn.LeakyReLU(inplace=True),      #Leaky ReLU activation function\n","            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1), #Fourth layer\n","            nn.BatchNorm2d(64),             #Batch normalization\n","            nn.LeakyReLU(inplace=True),     #Leaky ReLU activation function\n","            nn.MaxPool2d(kernel_size=2, stride=2),    #Max pooling\n","            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1), #Fifth layer\n","            nn.BatchNorm2d(64),            #Batch normalization\n","            nn.LeakyReLU(inplace=True),    #Leaky ReLU activation function\n","            nn.MaxPool2d(kernel_size=2, stride=2)    #Max pooling\n","          )\n","\n","        self.fc_layer = nn.Sequential(\n","            nn.Dropout(p=0.1),   #0.1 dropout rate\n","            nn.Linear(57600, 1000),\n","            nn.ReLU(inplace=True),\n","            nn.Linear(1000, 512),\n","            nn.ReLU(inplace=True),\n","            nn.Dropout(p=0.1),\n","            nn.Linear(512, 5) #outputs = number of classes\n","          )\n","\n","    def forward(self, x):\n","        # conv layers\n","        x = self.conv_layer(x)\n","        # flatten\n","        x = x.view(x.size(0), -1)\n","        #fc layer\n","        x = self.fc_layer(x)\n","        \n","        return x"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wS79XPC9huRE","outputId":"8e8a5bcc-8a0a-49c0-b786-f7efcfc401f3"},"outputs":[{"data":{"text/plain":["CNN(\n","  (conv_layer): Sequential(\n","    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (2): LeakyReLU(negative_slope=0.01, inplace=True)\n","    (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (5): LeakyReLU(negative_slope=0.01, inplace=True)\n","    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (7): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (8): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (9): LeakyReLU(negative_slope=0.01, inplace=True)\n","    (10): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (11): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (12): LeakyReLU(negative_slope=0.01, inplace=True)\n","    (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (14): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (15): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (16): LeakyReLU(negative_slope=0.01, inplace=True)\n","    (17): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  )\n","  (fc_layer): Sequential(\n","    (0): Dropout(p=0.1, inplace=False)\n","    (1): Linear(in_features=57600, out_features=1000, bias=True)\n","    (2): ReLU(inplace=True)\n","    (3): Linear(in_features=1000, out_features=512, bias=True)\n","    (4): ReLU(inplace=True)\n","    (5): Dropout(p=0.1, inplace=False)\n","    (6): Linear(in_features=512, out_features=5, bias=True)\n","  )\n",")"]},"execution_count":71,"metadata":{},"output_type":"execute_result"}],"source":["import torchvision.models as models\n","model = CNN()\n","#r'C:\\new downloads\\ourmodel.pth'\n","model.load_state_dict(torch.load(r'C:\\new downloads\\ourmodel.pth'))\n","model.eval()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tKxSZkcMeJvZ","outputId":"a91176c7-52a2-46f6-ba12-164fe26da9e5","scrolled":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Test Accuracy of the model on 249 female test images : 61.44578313253012 %\n","Test Accuracy of the model on 247 male test images : 58.06451612903226 %\n","Test Accuracy of the model on 100 mixed-gender test images : 59.73154362416108 %\n"]}],"source":["#model.eval()\n","with torch.no_grad():\n","  correct = 0\n","  total = 0\n","\n","for images, labels in test_loader1:\n","  outputs = model(images)\n","  _, predicted = torch.max(outputs.data, 1) #Computing the prediction of the model \n","  total += labels.size(0)\n","  correct += (predicted == labels).sum().item() #Computing the number of correct predicted items\n","print('Test Accuracy of the model on {} female test images : {} %'.format(len(sample_dataset1),(correct / total) * 100))\n","\n","for images, labels in test_loader2:\n","  outputs = model(images)\n","  _, predicted = torch.max(outputs.data, 1) #Computing the prediction of the model \n","  total += labels.size(0)\n","  correct += (predicted == labels).sum().item() #Computing the number of correct predicted items\n","print('Test Accuracy of the model on {} male test images : {} %'.format(len(sample_dataset2),(correct / total) * 100))\n","\n","for images, labels in test_loader3:\n","  outputs = model(images)\n","  _, predicted = torch.max(outputs.data, 1) #Computing the prediction of the model \n","  total += labels.size(0)\n","  correct += (predicted == labels).sum().item() #Computing the number of correct predicted items\n","print('Test Accuracy of the model on {} mixed-gender test images : {} %'.format(len(sample_dataset3),(correct / total) * 100))\n","\n"]},{"cell_type":"markdown","metadata":{"id":"BFk5_qBN7vLa"},"source":["## Classifying a single image"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"ooYwIRQB7vLc","outputId":"4e19c9aa-73bd-4f7e-b789-b0b2d6fecad0"},"outputs":[{"name":"stdout","output_type":"stream","text":["0\n","None\n"]}],"source":["from PIL import Image\n","classes = ('Cloth mask', 'N95 and FFP2', 'N95 and FFP2 with valve', 'Surgical mask','No mask')\n","loader = torchvision.transforms.Compose([\n","                #Resizing all of the imagesin the dataset to (240,240)\n","                transforms.Resize((240,240)),\n","                transforms.ToTensor(),\n","                #Normalization using the mean and variance over the whole dataset\n","                transforms.Normalize((0.5352, 0.5352, 0.5352),(0.3009, 0.3009, 0.3009))])\n","def classify(model, loader, path):\n","    \"\"\"load image, returns cuda tensor\"\"\"\n","    model = model.eval()\n","    image = Image.open(path)\n","    image = loader(image).float()\n","    image = Variable(image, requires_grad=True)\n","    image = image.unsqueeze(0)  #this is for VGG, may not be needed for ResNet\n","    output = model(image)\n","    _, predicted = torch.max(output.data, 1)\n","    print(predicted.item())\n","\n","print(classify(model= model, loader= loader, path = r'C:\\Concordia_Program\\AI\\project_part2\\Male_dataset\\0\\#cloth mask#_5.jpg') )\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Kz6vnJTO7vLe"},"outputs":[],"source":[""]}],"metadata":{"colab":{"name":"loading_model.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"nbformat":4,"nbformat_minor":0}