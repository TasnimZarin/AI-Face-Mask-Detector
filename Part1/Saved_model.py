# -*- coding: utf-8 -*-
"""Saved_model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mmpJNKmauKDys93RXQrJ3aZPBFdtLjhH
"""

import os
from os.path import exists
from pathlib import Path
import torch
from torch.utils.data import Dataset, DataLoader
import torchvision.transforms as transforms
import torchvision.datasets as datasets
from shutil import copyfile
import torchvision
from PIL import Image
import io
import random
import matplotlib.pyplot as plt
import numpy as np
from torch.autograd import Variable
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import random_split
from torch.utils.data import TensorDataset, ConcatDataset, DataLoader
import pandas as pd
from torchvision.transforms import ToTensor, Lambda
from torchvision.io import read_image
import matplotlib.pyplot as plt
import torch.optim as optim
from sklearn.metrics import accuracy_score,recall_score,precision_score,f1_score
from sklearn.metrics import plot_confusion_matrix
import sklearn
from skorch import NeuralNetClassifier

class CustomImageDataset():
    def __init__(self, annotations_file, img_dir,
            transform=torchvision.transforms.Compose([
                transforms.ToPILImage(),
                transforms.Grayscale(num_output_channels=3),
                transforms.ToTensor(),
                #Resizing all of the imagesin the dataset to (240,240)
                transforms.Resize((240,240)),
                #Normalization using the mean and variance over the whole dataset
                transforms.Normalize((0.5352, 0.5352, 0.5352),(0.3009, 0.3009, 0.3009))]),
                 #Lambda function is used for turning the integer into a tensor
            target_transform=Lambda(lambda y: torch.zeros(5, dtype=torch.float).scatter_(0, torch.tensor(y), value=1))):
        self.img_labels = pd.read_csv(annotations_file)   #Dataaset labels
        self.img_dir = img_dir      #Dataset image path     
        self.transform = transform   
        self.target_transform = target_transform
        

    def __len__(self):
        return len(self.img_labels)

    def __getitem__(self, idx):
        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])
        if(os.path.exists(img_path)):
            image = read_image(img_path)
            label = torch.tensor(int(self.img_labels.iloc[idx, 1]))
            if self.transform:
                image = self.transform(image)
            image.shape
            return image, label


batch_size = 16
validation_split = .2
shuffle_dataset = True
random_seed= 42   #Preventing from random events in each run
sample_dataset=CustomImageDataset(annotations_file=r'G:\.shortcut-targets-by-id\1noXdisFl6MYE_w2qtCawV4fozNQDCP_I\COMP-6721(AI_project)\Submission\sample-label.csv', img_dir=r'G:\.shortcut-targets-by-id\1noXdisFl6MYE_w2qtCawV4fozNQDCP_I\COMP-6721(AI_project)\Submission\Sample images')

test_loader = torch.utils.data.DataLoader(sample_dataset, batch_size,shuffle=False)

class CNN(nn.Module):
    def __init__(self):
        super(CNN, self).__init__()
        self.conv_layer = nn.Sequential(
            nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1), #First layer
            nn.BatchNorm2d(32),                 #Batch normalization
            nn.LeakyReLU(inplace=True),         #Leaky ReLU activation function
            nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, padding=1), #Second layer
            nn.BatchNorm2d(32),                #Batch normalization
            nn.LeakyReLU(inplace=True),        #Leaky ReLU activation function
            nn.MaxPool2d(kernel_size=2, stride=2),     #Max pooling
            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1), #Third layer
            nn.BatchNorm2d(64),              #Batch normalization
            nn.LeakyReLU(inplace=True),      #Leaky ReLU activation function
            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1), #Fourth layer
            nn.BatchNorm2d(64),             #Batch normalization
            nn.LeakyReLU(inplace=True),     #Leaky ReLU activation function
            nn.MaxPool2d(kernel_size=2, stride=2),    #Max pooling
            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1), #Fifth layer
            nn.BatchNorm2d(64),            #Batch normalization
            nn.LeakyReLU(inplace=True),    #Leaky ReLU activation function
            nn.MaxPool2d(kernel_size=2, stride=2)    #Max pooling
          )

        self.fc_layer = nn.Sequential(
            nn.Dropout(p=0.1),   #0.1 dropout rate
            nn.Linear(57600, 1000),
            nn.ReLU(inplace=True),
            nn.Linear(1000, 512),
            nn.ReLU(inplace=True),
            nn.Dropout(p=0.1),
            nn.Linear(512, 5) #outputs = number of classes
          )

    def forward(self, x):
        # conv layers
        x = self.conv_layer(x)
        # flatten
        x = x.view(x.size(0), -1)
        #fc layer
        x = self.fc_layer(x)
        
        return x

import torchvision.models as models
model = CNN()
model.load_state_dict(torch.load('ourmodel.pth'))
model.eval()

#model.eval()
with torch.no_grad():
  correct = 0
  total = 0

for images, labels in test_loader:
  outputs = model(images)
  _, predicted = torch.max(outputs.data, 1) #Computing the prediction of the model 
  total += labels.size(0)
  correct += (predicted == labels).sum().item() #Computing the number of correct predicted items
print('Test Accuracy of the model on the {} test images: {} %'.format(len(sample_dataset),(correct / total) * 100))
DEVICE = torch.device("cpu")
y_train = np.array([y for x, y in iter(sample_dataset)])
torch.manual_seed(0)
net = NeuralNetClassifier(
    CNN,
    max_epochs=1,
    lr=1e-3, #Learning rate
    batch_size=16,
    optimizer=optim.Adam, #Using adam optimizer
    criterion=nn.CrossEntropyLoss,
    device=DEVICE
)
net.fit(sample_dataset, y=y_train)
y_pred = net.predict(testset)  #Obtaining the predictions
y_test = np.array([y for x, y in iter(testset)]) #Obtaining labels of testset
print("accuracy: ",accuracy_score(y_test, y_pred)) #accuracy

plot_confusion_matrix(net, testset, y_test.reshape(-1, 1)) #Confusion matrix
plt.show()

print("y_pred: ",y_pred)

precision = sklearn.metrics.precision_score(y_test, y_pred, pos_label='positive', average='micro') #Precision
print('Precision : %.3f %%' % (100.0 * precision))
recall = sklearn.metrics.recall_score(y_test, y_pred, pos_label='positive', average='micro') #Recall
print('Recall : %.3f %%' % (100.0 * recall))
f1_score = sklearn.metrics.f1_score(y_test, y_pred, pos_label='positive', average='micro') #F1_score
print('F1_score : %.3f %%' % (100.0 * f1_score))
